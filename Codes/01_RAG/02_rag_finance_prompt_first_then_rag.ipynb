{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eff0a35",
   "metadata": {},
   "source": [
    "\n",
    "# Interroger un PDF financier avec OpenAI\n",
    "\n",
    " 1) *Prompt-only* avec **fitz** \n",
    "2) *RAG léger* (pypdf + embeddings).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cafcbe",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Pré‑requis\n",
    "Installez (une seule fois) :\n",
    "```bash\n",
    "pip install openai pymupdf pypdf numpy pandas\n",
    "```\n",
    "Définissez votre clé :\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai pymupdf pypdf numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c42ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fitz            # PyMuPDF\n",
    "from pypdf import PdfReader\n",
    "from openai import OpenAI\n",
    "\n",
    "# Imports communs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,4)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "# Détection  de la clé API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "print(\"OPENAI_API_KEY défini :\", api_key[:4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d53cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "pd.set_option(\"display.max_colwidth\", 160)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabfbde0",
   "metadata": {},
   "source": [
    "\n",
    "## A) Prompt-only avec **fitz** \n",
    "> Idéal pour **un document court** ou **une section précise** : on extrait un intervalle de pages et on l'injecte **tel quel** dans le prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd9734",
   "metadata": {},
   "source": [
    "\n",
    "### A.1 — Choisir le PDF et l'intervalle de pages\n",
    "- Mettez votre fichier PDF (ex. `rapport.pdf`).  \n",
    "- Par défaut ici : exemple `teslafinancialreport.pdf`.  \n",
    "- Définissez `PAGE_START`, `PAGE_END` et la limite de caractères `MAX_CHARS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PDF_PATH = \"data/teslafinancialreport.pdf\"   # ⬅️ remplacez par votre PDF\n",
    "PAGE_START = 1\n",
    "PAGE_END   = 6\n",
    "MAX_CHARS  = 12000\n",
    "\n",
    "doc = fitz.open(PDF_PATH)\n",
    "pages = []\n",
    "for i in range(doc.page_count):\n",
    "    page = doc.load_page(i)\n",
    "    txt = page.get_text(\"text\")\n",
    "    txt = re.sub(r\"[ \\t]+\", \" \", txt)\n",
    "    txt = re.sub(r\"\\s*\\n\\s*\", \"\\n\", txt).strip()\n",
    "    pages.append({\"page\": i+1, \"text\": txt})\n",
    "\n",
    "df_fitx = pd.DataFrame(pages)\n",
    "print(\"Pages lues (fitz) :\", len(df_fitx))\n",
    "df_fitx.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41904d45",
   "metadata": {},
   "source": [
    "\n",
    "### A.2 — Construire le contexte et poser la question\n",
    "Le modèle répond **uniquement** à partir du contexte fourni et **cite les pages** avec `(p.X)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b650f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blocks = []\n",
    "for p in range(PAGE_START, PAGE_END + 1):\n",
    "    row = df_fitx[df_fitx[\"page\"] == p].iloc[0]\n",
    "    blocks.append(f\"[PAGE {p}]\\n{row['text']}\")\n",
    "CONTEXT_PO = \"\\n\\n---\\n\".join(blocks)[:MAX_CHARS]\n",
    "\n",
    "QUESTION_PO = \"Fais un résumé clair des points saillants financiers et opérationnels, en citant les pages (p.X).\"\n",
    "\n",
    "SYSTEM_PO = (\n",
    "    \"Vous êtes analyste financier. Répondez uniquement à partir du CONTEXTE fourni. \"\n",
    "    \"Citez systématiquement les pages (p.X). \"\n",
    "    \"Si l'information manque, répondez : 'Non trouvé dans le contexte fourni'.\"\n",
    ")\n",
    "\n",
    "USER_PO = f\"\"\"QUESTION :\n",
    "{QUESTION_PO}\n",
    "\n",
    "CONTEXTE (pages {PAGE_START}–{PAGE_END}) :\n",
    "{CONTEXT_PO}\n",
    "\"\"\"\n",
    "\n",
    "resp_po = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\":\"system\",\"content\":SYSTEM_PO},\n",
    "              {\"role\":\"user\",\"content\":USER_PO}],\n",
    "    temperature=0.2,\n",
    ")\n",
    "answer_po = resp_po.choices[0].message.content\n",
    "print(answer_po)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722f73d",
   "metadata": {},
   "source": [
    "\n",
    "### A.3 — Variantes de questions \n",
    "- *Quels sont les risques majeurs évoqués et leurs impacts financiers ?*  \n",
    "- *Comment évoluent revenus et marges ; quelles explications sont données ?*  \n",
    "- *Quelles perspectives (guidance, capex, drivers) ?*  \n",
    "- *Quels éléments de gouvernance/compliance sont mentionnés ?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee98418",
   "metadata": {},
   "source": [
    "\n",
    "## B) RAG léger (pypdf + embeddings OpenAI)\n",
    "> Pour **documents longs** : segmentation en *chunks*, embeddings, récupération des meilleurs extraits, réponse **sourcée**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81407cd5",
   "metadata": {},
   "source": [
    "\n",
    "### B.1 — Importer & normaliser le PDF (pypdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reader = PdfReader(PDF_PATH)\n",
    "pages = []\n",
    "for i, page in enumerate(reader.pages, start=1):\n",
    "    txt = page.extract_text() or \"\"\n",
    "    txt = re.sub(r\"[ \\t]+\", \" \", txt)\n",
    "    txt = re.sub(r\"\\s*\\n\\s*\", \"\\n\", txt).strip()\n",
    "    pages.append({\"page\": i, \"text\": txt})\n",
    "\n",
    "df_pages = pd.DataFrame(pages)\n",
    "print(\"Pages lues (pypdf) :\", len(df_pages))\n",
    "df_pages.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d6011",
   "metadata": {},
   "source": [
    "\n",
    "### B.2 — Segmenter en *chunks* (fenêtre + chevauchement)\n",
    "- Taille cible : **1500** caractères ; chevauchement **400** caractères.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8147023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CHUNK_SIZE = 1500\n",
    "CHUNK_OVERLAP = 400\n",
    "\n",
    "chunks = []\n",
    "for _, row in df_pages.iterrows():\n",
    "    p = int(row[\"page\"]); t = row[\"text\"]\n",
    "    if not t:\n",
    "        continue\n",
    "    s = 0\n",
    "    while s < len(t):\n",
    "        e = s + CHUNK_SIZE\n",
    "        chunks.append({\"page\": p, \"start\": s, \"end\": min(e, len(t)), \"text\": t[s:e]})\n",
    "        s += CHUNK_SIZE - CHUNK_OVERLAP\n",
    "\n",
    "df_chunks = pd.DataFrame(chunks)\n",
    "print(\"Chunks créés :\", len(df_chunks))\n",
    "df_chunks.sample(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f582b",
   "metadata": {},
   "source": [
    "\n",
    "### B.3 — Embeddings & index en mémoire\n",
    "Modèle recommandé : `text-embedding-3-small`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c5692",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "texts = df_chunks[\"text\"].tolist()\n",
    "emb = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "MAT = np.array([e.embedding for e in emb.data], dtype=np.float32)\n",
    "print(\"Matrice embeddings :\", MAT.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a36839",
   "metadata": {},
   "source": [
    "\n",
    "### B.4 — Question → similarité cosinus → top‑k extraits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_scores(mat: np.ndarray, q: np.ndarray) -> np.ndarray:\n",
    "    denom = (np.linalg.norm(mat, axis=1) * np.linalg.norm(q) + 1e-8)\n",
    "    return (mat @ q) / denom\n",
    "\n",
    "QUESTION = \"Quels sont les principaux risques évoqués et leurs impacts financiers ?\"\n",
    "\n",
    "q_vec = np.array(client.embeddings.create(model=EMBED_MODEL, input=[QUESTION]).data[0].embedding, dtype=np.float32)\n",
    "scores = cosine_scores(MAT, q_vec)\n",
    "\n",
    "TOP_K = 6\n",
    "idx = np.argsort(-scores)[:TOP_K]\n",
    "df_top = df_chunks.iloc[idx].copy()\n",
    "df_top[\"score\"] = scores[idx]\n",
    "df_top = df_top.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "df_top[[\"page\",\"score\",\"text\"]].head(TOP_K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d971b",
   "metadata": {},
   "source": [
    "\n",
    "### B.5 — Construire la réponse **sourcée** (citer les pages)\n",
    "- Répondre **uniquement** à partir du **CONTEXTE**.  \n",
    "- Citer systématiquement les pages avec `(p.X)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82dc4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blocks = []\n",
    "for _, r in df_top.iterrows():\n",
    "    blocks.append(f\"[PAGE {int(r['page'])}]\\n{r['text']}\")\n",
    "CONTEXT = \"\\n\\n---\\n\".join(blocks)\n",
    "\n",
    "SYSTEM = (\n",
    "    \"Vous êtes analyste financier. Répondez en français, de façon concise et sourcée. \"\n",
    "    \"N'utilisez QUE le CONTEXTE fourni. Citez systématiquement les pages (p.X). \"\n",
    "    \"Si l'information n'est pas dans le contexte, répondez : 'Non trouvé dans le contexte fourni'.\"\n",
    ")\n",
    "\n",
    "USER = f\"\"\"QUESTION :\n",
    "{QUESTION}\n",
    "\n",
    "CONTEXTE (extraits du PDF) :\n",
    "{CONTEXT}\n",
    "\"\"\"\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\":\"system\",\"content\":SYSTEM},\n",
    "              {\"role\":\"user\",\"content\":USER}],\n",
    "    temperature=0.2,\n",
    ")\n",
    "answer = resp.choices[0].message.content\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da292d5d",
   "metadata": {},
   "source": [
    "\n",
    "## C) Banque de questions & conseils\n",
    "**Questions génériques :** marges, FCF, liquidité, risques, dette, capex, guidance, gouvernance.  \n",
    "**Conseils :** documents longs → RAG ; sections ciblées → prompt-only. Toujours **citer les pages**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b46fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Gradio — 3 approches (OpenAI) avec chemins relatifs \"data/...\"\n",
    "# 1) Prompt-only (fitz) : intervalle de pages injecté tel quel\n",
    "# 2) RAG léger (PDF dans data/...) : chunks + embeddings + top-k\n",
    "# 3) RAG léger (PDF uploadé)\n",
    "#\n",
    "# Prérequis :\n",
    "#   pip install gradio openai pymupdf pypdf numpy pandas\n",
    "#   export OPENAI_API_KEY=\"sk-...\"\n",
    "# ============================================================\n",
    "\n",
    "import os, re, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fitz                      # PyMuPDF\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "\n",
    "# === Config \"relative path\" ===\n",
    "DATA_DIR    = \"data\"   # racine locale (ex: data/tesla/...)\n",
    "DEFAULT_PDF = \"data/teslafinancialreport.pdf\"  # ⬅️ valeur par défaut demandée\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "CHAT_MODEL  = \"gpt-4o-mini\"\n",
    "\n",
    "# === Client OpenAI (exige OPENAI_API_KEY) ===\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# ========== Helpers communs (sans try/except) =================================\n",
    "def extract_pdf_pages_fitz(path: str) -> pd.DataFrame:\n",
    "    doc = fitz.open(path)\n",
    "    rows = []\n",
    "    for i in range(doc.page_count):\n",
    "        page = doc.load_page(i)\n",
    "        txt  = page.get_text(\"text\")\n",
    "        txt  = re.sub(r\"[ \\t]+\", \" \", txt)\n",
    "        txt  = re.sub(r\"\\s*\\n\\s*\", \"\\n\", txt).strip()\n",
    "        rows.append({\"page\": i+1, \"text\": txt})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def extract_pdf_pages_pypdf(path: str) -> pd.DataFrame:\n",
    "    reader = PdfReader(path)\n",
    "    rows = []\n",
    "    for i, page in enumerate(reader.pages, start=1):\n",
    "        txt = page.extract_text() or \"\"\n",
    "        txt = re.sub(r\"[ \\t]+\", \" \", txt)\n",
    "        txt = re.sub(r\"\\s*\\n\\s*\", \"\\n\", txt).strip()\n",
    "        rows.append({\"page\": i, \"text\": txt})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def build_chunks(df_pages: pd.DataFrame, chunk_size: int = 1500, overlap: int = 400) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, r in df_pages.iterrows():\n",
    "        p = int(r[\"page\"]); t = r[\"text\"]\n",
    "        if not t: \n",
    "            continue\n",
    "        s = 0; step = max(1, chunk_size - overlap)\n",
    "        while s < len(t):\n",
    "            e = s + chunk_size\n",
    "            rows.append({\"page\": p, \"start\": s, \"end\": min(e, len(t)), \"text\": t[s:e]})\n",
    "            s += step\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def embed_texts(texts: list[str]) -> np.ndarray:\n",
    "    out = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    return np.array([e.embedding for e in out.data], dtype=np.float32)\n",
    "\n",
    "def embed_query(q: str) -> np.ndarray:\n",
    "    v = client.embeddings.create(model=EMBED_MODEL, input=[q]).data[0].embedding\n",
    "    return np.array(v, dtype=np.float32)\n",
    "\n",
    "def cosine_scores(mat: np.ndarray, qvec: np.ndarray) -> np.ndarray:\n",
    "    denom = (np.linalg.norm(mat, axis=1) * np.linalg.norm(qvec) + 1e-8)\n",
    "    return (mat @ qvec) / denom\n",
    "\n",
    "def build_context_from_rows(df_rows: pd.DataFrame, max_chars: int | None = None) -> str:\n",
    "    ctx = \"\\n\\n---\\n\".join([f\"[PAGE {int(r['page'])}]\\n{r['text']}\" for _, r in df_rows.iterrows()])\n",
    "    return ctx[:max_chars] if max_chars else ctx\n",
    "\n",
    "def answer_from_context(question: str, context: str, model: str = CHAT_MODEL, temperature: float = 0.2) -> str:\n",
    "    SYSTEM = (\n",
    "        \"Vous êtes analyste financier. Répondez en français, de façon concise et sourcée. \"\n",
    "        \"N'utilisez QUE le CONTEXTE fourni. Citez systématiquement les pages (p.X). \"\n",
    "        \"Si l'information n'est pas dans le contexte, répondez : 'Non trouvé dans le contexte fourni'.\"\n",
    "    )\n",
    "    USER = f\"QUESTION :\\n{question}\\n\\nCONTEXTE :\\n{context}\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"system\",\"content\":SYSTEM},\n",
    "                  {\"role\":\"user\",  \"content\":USER}],\n",
    "        temperature=float(temperature),\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "def list_pdfs_recursive(root: str = DATA_DIR) -> list[str]:\n",
    "    # Renvoie des chemins RELATIFS : \"data/xxx/rapport.pdf\" ou \"data/rapport.pdf\"\n",
    "    if not os.path.isdir(root):\n",
    "        return []\n",
    "    found = sorted(glob.glob(os.path.join(root, \"**\", \"*.pdf\"), recursive=True))\n",
    "    return [os.path.relpath(p, start=\".\") for p in found]\n",
    "\n",
    "# ========== Handlers ==========================================================\n",
    "# 1) PROMPT-ONLY (fitz)\n",
    "def handle_prompt_only(pdf_rel_path: str, page_start: int, page_end: int, max_chars: int,\n",
    "                       question: str, model: str, temperature: float):\n",
    "    df = extract_pdf_pages_fitz(pdf_rel_path)\n",
    "    page_start = max(1, int(page_start))\n",
    "    page_end   = min(int(page_end), int(df[\"page\"].max()))\n",
    "    rows = df[(df[\"page\"] >= page_start) & (df[\"page\"] <= page_end)].copy()\n",
    "    context = build_context_from_rows(rows, max_chars=max_chars)\n",
    "    answer  = answer_from_context(question, context, model=model, temperature=temperature)\n",
    "    pages_used = sorted(rows[\"page\"].unique().tolist())\n",
    "    return answer, f\"Pages dans le contexte : {pages_used}\", context\n",
    "\n",
    "# 2) RAG LÉGER — PDF dans data/\n",
    "def handle_rag_from_data(pdf_rel_path: str, question: str,\n",
    "                         chunk_size: int, overlap: int, top_k: int,\n",
    "                         embed_model: str, chat_model: str, temperature: float):\n",
    "    global EMBED_MODEL, CHAT_MODEL\n",
    "    EMBED_MODEL = embed_model\n",
    "    CHAT_MODEL  = chat_model\n",
    "\n",
    "    df_pages  = extract_pdf_pages_pypdf(pdf_rel_path)\n",
    "    df_chunks = build_chunks(df_pages, chunk_size=chunk_size, overlap=overlap)\n",
    "\n",
    "    M = embed_texts(df_chunks[\"text\"].tolist())\n",
    "    q = embed_query(question)\n",
    "    scores = cosine_scores(M, q)\n",
    "    idx    = np.argsort(-scores)[:int(top_k)]\n",
    "    df_top = df_chunks.iloc[idx].copy()\n",
    "    df_top[\"score\"] = scores[idx]\n",
    "    df_top = df_top.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    context = build_context_from_rows(df_top, max_chars=None)\n",
    "    answer  = answer_from_context(question, context, model=CHAT_MODEL, temperature=temperature)\n",
    "    pages_used = sorted(df_top[\"page\"].unique().tolist())\n",
    "    return answer, f\"Pages retenues (top-{top_k}) : {pages_used}\", context\n",
    "\n",
    "# 3) RAG LÉGER — PDF uploadé\n",
    "def handle_rag_upload(file_obj, question: str,\n",
    "                      chunk_size: int, overlap: int, top_k: int,\n",
    "                      embed_model: str, chat_model: str, temperature: float):\n",
    "    global EMBED_MODEL, CHAT_MODEL\n",
    "    EMBED_MODEL = embed_model\n",
    "    CHAT_MODEL  = chat_model\n",
    "\n",
    "    path = file_obj.name\n",
    "    df_pages  = extract_pdf_pages_pypdf(path)\n",
    "    df_chunks = build_chunks(df_pages, chunk_size=chunk_size, overlap=overlap)\n",
    "\n",
    "    M = embed_texts(df_chunks[\"text\"].tolist())\n",
    "    q = embed_query(question)\n",
    "    scores = cosine_scores(M, q)\n",
    "    idx    = np.argsort(-scores)[:int(top_k)]\n",
    "    df_top = df_chunks.iloc[idx].copy()\n",
    "    df_top[\"score\"] = scores[idx]\n",
    "    df_top = df_top.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    context = build_context_from_rows(df_top, max_chars=None)\n",
    "    answer  = answer_from_context(question, context, model=CHAT_MODEL, temperature=temperature)\n",
    "    pages_used = sorted(df_top[\"page\"].unique().tolist())\n",
    "    return answer, f\"Pages retenues (top-{top_k}) : {pages_used}\", context\n",
    "\n",
    "# ========== Construire l'UI ===================================================\n",
    "pdf_options = list_pdfs_recursive(DATA_DIR)\n",
    "default_choice = DEFAULT_PDF if DEFAULT_PDF in pdf_options else (pdf_options[0] if pdf_options else \"data/\")\n",
    "\n",
    "with gr.Blocks(fill_height=True) as demo:\n",
    "    gr.Markdown(\"# Chat PDF financier — 3 approches (OpenAI)\")\n",
    "\n",
    "    # --- 1) Prompt-only (fitz) ---\n",
    "    with gr.Tab(\"1) Prompt-only (fitz)\"):\n",
    "        gr.Markdown(\"**Idéal pour une section précise** (pas d’embeddings).\")\n",
    "        with gr.Row():\n",
    "            po_pdf   = gr.Textbox(label=\"Chemin PDF (relatif)\", value=DEFAULT_PDF)\n",
    "            po_pbeg  = gr.Slider(label=\"Page début\", minimum=1, maximum=1000, step=1, value=1)\n",
    "            po_pend  = gr.Slider(label=\"Page fin\",   minimum=1, maximum=1000, step=1, value=6)\n",
    "        with gr.Row():\n",
    "            po_maxc  = gr.Slider(label=\"Max caractères injectés\", minimum=2000, maximum=40000, step=1000, value=12000)\n",
    "            po_model = gr.Dropdown(label=\"Modèle chat\", choices=[\"gpt-4o-mini\",\"gpt-4o\",\"gpt-4o-reasoning\"], value=CHAT_MODEL)\n",
    "            po_temp  = gr.Slider(label=\"Température\", minimum=0.0, maximum=1.0, step=0.1, value=0.2)\n",
    "        po_q = gr.Textbox(label=\"Question\", value=\"Fais un résumé clair des points saillants financiers et opérationnels, en citant les pages (p.X).\", lines=3)\n",
    "        po_btn = gr.Button(\"Analyser (prompt-only)\")\n",
    "        with gr.Row():\n",
    "            po_answer = gr.Textbox(label=\"Réponse\", lines=10)\n",
    "            po_meta   = gr.Textbox(label=\"Infos (pages utilisées)\")\n",
    "        po_ctx = gr.Code(label=\"Contexte injecté\", language=\"markdown\")\n",
    "\n",
    "        po_btn.click(\n",
    "            handle_prompt_only,\n",
    "            inputs=[po_pdf, po_pbeg, po_pend, po_maxc, po_q, po_model, po_temp],\n",
    "            outputs=[po_answer, po_meta, po_ctx]\n",
    "        )\n",
    "\n",
    "    # --- 2) RAG léger — PDF dans data/ ---\n",
    "    with gr.Tab(\"2) RAG léger — PDF dans data/\"):\n",
    "        gr.Markdown(\"Sélectionnez un PDF présent dans **data/** (et sous-dossiers).\")\n",
    "        with gr.Row():\n",
    "            rag_pdf   = gr.Dropdown(label=\"Fichier PDF\", choices=pdf_options, value=default_choice)\n",
    "            rag_q     = gr.Textbox(label=\"Question\", value=\"Quels sont les principaux risques évoqués et leurs impacts financiers ?\", lines=3)\n",
    "        with gr.Row():\n",
    "            rag_chunk = gr.Slider(label=\"Taille chunk\", minimum=600, maximum=3000, step=100, value=1500)\n",
    "            rag_ovlp  = gr.Slider(label=\"Chevauchement\", minimum=100, maximum=800, step=50, value=400)\n",
    "            rag_topk  = gr.Slider(label=\"top-k\", minimum=2, maximum=10, step=1, value=6)\n",
    "        with gr.Row():\n",
    "            rag_embed = gr.Dropdown(label=\"Modèle embeddings\", choices=[\"text-embedding-3-small\",\"text-embedding-3-large\"], value=EMBED_MODEL)\n",
    "            rag_chat  = gr.Dropdown(label=\"Modèle chat\", choices=[\"gpt-4o-mini\",\"gpt-4o\",\"gpt-4o-reasoning\"], value=CHAT_MODEL)\n",
    "            rag_temp  = gr.Slider(label=\"Température\", minimum=0.0, maximum=1.0, step=0.1, value=0.2)\n",
    "\n",
    "        def refresh_list():\n",
    "            files = list_pdfs_recursive(DATA_DIR)\n",
    "            return gr.update(choices=files, value=(DEFAULT_PDF if DEFAULT_PDF in files else (files[0] if files else \"\")))\n",
    "\n",
    "        refresh_btn = gr.Button(\"🔄 Rafraîchir la liste\")\n",
    "        refresh_btn.click(fn=refresh_list, inputs=None, outputs=rag_pdf)\n",
    "\n",
    "        rag_btn = gr.Button(\"Analyser (RAG léger — data)\")\n",
    "        with gr.Row():\n",
    "            rag_answer = gr.Textbox(label=\"Réponse\", lines=10)\n",
    "            rag_meta   = gr.Textbox(label=\"Infos (pages top-k)\")\n",
    "        rag_ctx = gr.Code(label=\"Contexte (extraits top-k)\", language=\"markdown\")\n",
    "\n",
    "        rag_btn.click(\n",
    "            handle_rag_from_data,\n",
    "            inputs=[rag_pdf, rag_q, rag_chunk, rag_ovlp, rag_topk, rag_embed, rag_chat, rag_temp],\n",
    "            outputs=[rag_answer, rag_meta, rag_ctx]\n",
    "        )\n",
    "\n",
    "    # --- 3) RAG léger — PDF uploadé ---\n",
    "    with gr.Tab(\"3) RAG léger — PDF uploadé\"):\n",
    "        gr.Markdown(\"Importez votre PDF et posez votre question.\")\n",
    "        with gr.Row():\n",
    "            up_file = gr.File(label=\"PDF\")\n",
    "            up_q    = gr.Textbox(label=\"Question\", value=\"Quels sont les principaux risques évoqués et leurs impacts financiers ?\", lines=3)\n",
    "        with gr.Row():\n",
    "            up_chunk = gr.Slider(label=\"Taille chunk\", minimum=600, maximum=3000, step=100, value=1500)\n",
    "            up_ovlp  = gr.Slider(label=\"Chevauchement\", minimum=100, maximum=800, step=50, value=400)\n",
    "            up_topk  = gr.Slider(label=\"top-k\", minimum=2, maximum=10, step=1, value=6)\n",
    "        with gr.Row():\n",
    "            up_embed = gr.Dropdown(label=\"Modèle embeddings\", choices=[\"text-embedding-3-small\",\"text-embedding-3-large\"], value=EMBED_MODEL)\n",
    "            up_chat  = gr.Dropdown(label=\"Modèle chat\", choices=[\"gpt-4o-mini\",\"gpt-4o\",\"gpt-4o-reasoning\"], value=CHAT_MODEL)\n",
    "            up_temp  = gr.Slider(label=\"Température\", minimum=0.0, maximum=1.0, step=0.1, value=0.2)\n",
    "\n",
    "        up_btn = gr.Button(\"Analyser (RAG léger — upload)\")\n",
    "        with gr.Row():\n",
    "            up_answer = gr.Textbox(label=\"Réponse\", lines=10)\n",
    "            up_meta   = gr.Textbox(label=\"Infos (pages top-k)\")\n",
    "        up_ctx = gr.Code(label=\"Contexte (extraits top-k)\", language=\"markdown\")\n",
    "\n",
    "        up_btn.click(\n",
    "            handle_rag_upload,\n",
    "            inputs=[up_file, up_q, up_chunk, up_ovlp, up_topk, up_embed, up_chat, up_temp],\n",
    "            outputs=[up_answer, up_meta, up_ctx]\n",
    "        )\n",
    "\n",
    "# Lancer l’interface locale\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
